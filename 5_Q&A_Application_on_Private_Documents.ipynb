{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZJmAw7xYPPTywADd8JA6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rregmi1993/LLM-large-language-model/blob/main/5_Q%26A_Application_on_Private_Documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPQaOKAbzc0m",
        "outputId": "c7af6fcc-224a-40a2-89e5-2bf15f3fd42e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai (from -r requirement.txt (line 1))\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain (from -r requirement.txt (line 2))\n",
            "  Downloading langchain-0.0.352-py3-none-any.whl (794 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.4/794.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pinecone-client (from -r requirement.txt (line 3))\n",
            "  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv (from -r requirement.txt (line 4))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting tiktoken (from -r requirement.txt (line 5))\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llmx in /usr/local/lib/python3.10/dist-packages (from -r requirement.txt (line 6)) (0.0.15a0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirement.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r requirement.txt (line 1)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r requirement.txt (line 1))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirement.txt (line 1)) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r requirement.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirement.txt (line 1)) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai->-r requirement.txt (line 1))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirement.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirement.txt (line 2)) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirement.txt (line 2)) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirement.txt (line 2)) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain->-r requirement.txt (line 2))\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain->-r requirement.txt (line 2))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain->-r requirement.txt (line 2))\n",
            "  Downloading langchain_community-0.0.6-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain->-r requirement.txt (line 2))\n",
            "  Downloading langchain_core-0.1.3-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.70 (from langchain->-r requirement.txt (line 2))\n",
            "  Downloading langsmith-0.0.75-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirement.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirement.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirement.txt (line 2)) (8.2.3)\n",
            "Collecting loguru>=0.5.0 (from pinecone-client->-r requirement.txt (line 3))\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython>=2.0.0 (from pinecone-client->-r requirement.txt (line 3))\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r requirement.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r requirement.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->-r requirement.txt (line 5)) (2023.6.3)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from llmx->-r requirement.txt (line 6)) (5.6.3)\n",
            "Collecting cohere (from llmx->-r requirement.txt (line 6))\n",
            "  Downloading cohere-4.39-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google.auth in /usr/local/lib/python3.10/dist-packages (from llmx->-r requirement.txt (line 6)) (2.17.3)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from llmx->-r requirement.txt (line 6)) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirement.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirement.txt (line 1)) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirement.txt (line 2))\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirement.txt (line 2))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r requirement.txt (line 1)) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirement.txt (line 1))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirement.txt (line 1))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain->-r requirement.txt (line 2))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain->-r requirement.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client->-r requirement.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r requirement.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirement.txt (line 2)) (3.0.2)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere->llmx->-r requirement.txt (line 6))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere->llmx->-r requirement.txt (line 6))\n",
            "  Downloading fastavro-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib_metadata<7.0,>=6.0 (from cohere->llmx->-r requirement.txt (line 6))\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google.auth->llmx->-r requirement.txt (line 6)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google.auth->llmx->-r requirement.txt (line 6)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google.auth->llmx->-r requirement.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer->llmx->-r requirement.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere->llmx->-r requirement.txt (line 6)) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google.auth->llmx->-r requirement.txt (line 6)) (0.5.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirement.txt (line 2))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: typing-extensions, python-dotenv, mypy-extensions, marshmallow, loguru, jsonpointer, importlib_metadata, h11, fastavro, dnspython, backoff, typing-inspect, tiktoken, pinecone-client, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, cohere, openai, langchain-core, langchain-community, langchain\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 cohere-4.39 dataclasses-json-0.6.3 dnspython-2.4.2 fastavro-1.9.2 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 importlib_metadata-6.11.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.352 langchain-community-0.0.6 langchain-core-0.1.3 langsmith-0.0.75 loguru-0.7.2 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-1.6.1 pinecone-client-2.2.4 python-dotenv-1.0.0 tiktoken-0.5.2 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirement.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb7d55acz7uu",
        "outputId": "4848fe83-55df-4395-d721-a298f3421266"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh4amy9b0qn2",
        "outputId": "fe16e1b0-34ac-496a-c775-bfe8eb7c9b99"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/277.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/277.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/277.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/277.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.9/277.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlxXGHXb5ek-",
        "outputId": "03c1afb0-b48a-4d38-d31e-8c137cc06017"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "import os\n",
        "def load_documents(file):\n",
        "  name, extension = os.path.splitext(file)\n",
        "\n",
        "  if extension == '.pdf':\n",
        "    print(f'loading a file .......{file}')\n",
        "    loader = PyPDFLoader(file)\n",
        "\n",
        "  elif extension == '.docx':\n",
        "    print(f'loading a file .......{file}')\n",
        "    loader = Docx2txtLoader(file)\n",
        "\n",
        "  else:\n",
        "    print(f\"give file {file} format is not supported by application\")\n",
        "    return None\n",
        "\n",
        "  data = loader.load()\n",
        "  print(\"completed ....\")\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "mw6LK6JG0vEo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "def chunk_data(data, chunk_size=256):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap =0)\n",
        "  chunks = text_splitter.split_documents(data)\n",
        "  return chunks\n",
        "\n"
      ],
      "metadata": {
        "id": "M6lS08sZ7fIN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding and uploading to a vector database\n",
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "def insert_or_fetch_embedding(index_name):\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "  pinecone.init(api_key = os.environ.get('PINECONE_API_KEY'), environment= os.environ.get('PINECONE_ENV'))\n",
        "  if index_name in pinecone.list_indexes():\n",
        "    print(f'Index {index_name} already exit. loading embedding ..', end='')\n",
        "    vectore_store = Pinecone.from_existing_index(index_name, embeddings)\n",
        "    print(\"Done\")\n",
        "  else:\n",
        "    print(f'Creating index {index_name} and embedding ...', end='')\n",
        "    pinecone.create_index(index_name, dimension=1536, metric='cosine')\n",
        "    vectore_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
        "    print('Done')\n",
        "\n",
        "    return vectore_store\n"
      ],
      "metadata": {
        "id": "49b6RTHCfclz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "def delete_pinecone_index(index_name='all'):\n",
        "  pinecone.init(api_key = os.environ.get('PINECONE_API_KEY'), environment= os.environ.get('PINECONE_ENV'))\n",
        "  if index_name =='all':\n",
        "    indexes = pinecone.list_indexes()\n",
        "    print('deleting all indexes....')\n",
        "    for index in indexes:\n",
        "      pinecone.delete_index(index)\n",
        "    print('Done')\n",
        "  else:\n",
        "    print(f'deleting index {index_name}...', end='')\n",
        "    pinecone.delete_index(index_name)\n"
      ],
      "metadata": {
        "id": "UdaW-Fr2iy4q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove content if you are running from the local jupyter notebook as i am running colab its by default taking /content\n",
        "data = load_documents('/content/documents/VideoPoet.pdf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOkMOTza1Rza",
        "outputId": "4506beed-cbad-4e54-f584-2f7a42ccba7f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading a file ......./content/documents/VideoPoet.pdf\n",
            "completed ....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(data[1].page_content)\n",
        "#print(data[1].metadata)"
      ],
      "metadata": {
        "id": "vpzuAIEa2sR7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the pages in your file\n",
        "print(f'you have {len(data)} page in your data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG_AqG1j3ls_",
        "outputId": "5d6a6fb0-c199-4448-8123-6b0c7e8b7e09"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you have 20 page in your data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count the number of characters in the page\n",
        "print(f'There are {len(data[1].page_content)} characters in the page')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSIs0BWG37XI",
        "outputId": "db58ad17-59fb-432d-a3c5-9e6027e82998"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2266 characters in the page\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chunks load\n",
        "chunks = chunk_data(data)"
      ],
      "metadata": {
        "id": "KPoIL_9z4MFN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wPNiznp8XYR",
        "outputId": "413e6bba-04aa-4cbd-be95-928972b4f765"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#delete index\n",
        "delete_pinecone_index()"
      ],
      "metadata": {
        "id": "g3DUmFza8eSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73b62a7-e329-4e7f-fa84-26bd89e2c974"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deleting all indexes....\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'askprivatedocumnet'\n",
        "vector_store = insert_or_fetch_embedding(index_name)"
      ],
      "metadata": {
        "id": "Cx9GCf1Aj_-E",
        "outputId": "7962c3fa-5766-4995-c701-aef4b17b275f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating index askprivatedocumnet and embedding ...Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "def ask_question_for_ans(vector_store, qns):\n",
        "  llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
        "  retriver = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
        "  chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriver)\n",
        "  answer = chain.run(qns)\n",
        "  return answer\n",
        "\n",
        "\n",
        "\n",
        "#with memory of above method\n",
        "#from langchain.memory import ConversationBufferMemory\n",
        "#from langchain.prompts import MessagesPlaceholder\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "def ask_with_memory(vector_store, qns, chat_history=[]):\n",
        "  llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
        "  retriver = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
        "  crc = ConversationalRetrievalChain.from_llm(llm, retriver)\n",
        "  result = crc({'question': qns, 'chat_history': chat_history})\n",
        "  chat_history.append((qns, result['answer']))\n",
        "\n",
        "  return result, chat_history\n"
      ],
      "metadata": {
        "id": "4gykxusak0YR"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qns = 'what is the whole document about ?'\n",
        "answer = ask_question_for_ans(vector_store, qns)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "4vqbPBxyo4HM",
        "outputId": "9ec10c4b-eba0-48cd-c426-e0cb909330e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document discusses a model called VideoPoet that is capable of understanding and generating large-scale motions in videos based on text prompts. The document also highlights the potential of using large language adaption or finetuning to perform various tasks. The evaluation of the model on different tasks is discussed, including its performance on the K600 and SSv2 datasets, as well as the text-to-video task. The model is shown to have a zero-shot capability by chaining individual tasks together.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "i = 1\n",
        "print('enter Quit or Exit to quit:::')\n",
        "while True:\n",
        "  qns = input(f'question # {i}: ')\n",
        "  i += 1\n",
        "  if qns.lower() in ['quit', 'exit']:\n",
        "    print('Exiting ...... bye')\n",
        "    time.sleep(2)\n",
        "    break\n",
        "\n",
        "  answer = ask_question_for_ans(vector_store, qns)\n",
        "  print(f'\\n Answer: {answer}')\n",
        "  print('---------------------------------------------------------')"
      ],
      "metadata": {
        "id": "jWoOIiuYpCrE",
        "outputId": "981d91fb-3783-4d01-ab79-d57839e18b00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter Quit or Exit to quit:::\n",
            "question # 1: summaries the main finding of this research paper ?\n",
            "\n",
            " Answer: The main findings of this research paper are summarized as follows: The authors retrained their 1B and 8B models using task design and text-paired training data. They compared the performance of these two pretrained models and found that the 8B model outperformed the 1B model in various dimensions. The proportions of successful trials were represented by green, gray, and pink bars in Figure 6.\n",
            "---------------------------------------------------------\n",
            "question # 2: describe VideoPoet algorithm\n",
            "\n",
            " Answer: The description of the VideoPoet algorithm is not provided in the given context.\n",
            "---------------------------------------------------------\n",
            "question # 3: summaries the comparision study\n",
            "\n",
            " Answer: The comparison study evaluated VideoPoet against recent leading text-to-video generative models. The evaluation was done through human side-by-side evaluations. The results showed the proportion of trials where VideoPoet was preferred over an alternative, similar to, or comparable model. However, due to the different visual domains and training subsets used in the models, it was difficult to identify consistent patterns in the results. Additionally, a separate evaluation was done to assess the CLIP similarity score and human preference on video stylization, using a set of 20 videos from DA VIS 2016 and prompts. Further details about the specific results of the comparison study are not provided in the given context.\n",
            "---------------------------------------------------------\n",
            "question # 4: exit\n",
            "Exiting ...... bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOZV_iftqEKg"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#asking with memory\n",
        "chat_history = []\n",
        "qns = 'what is the finding of this research paper'\n",
        "result, chat_history = ask_with_memory(vector_store, qns, chat_history)\n",
        "print(result['answer'])\n",
        "print(chat_history)"
      ],
      "metadata": {
        "id": "vGwRgZR0rcnV",
        "outputId": "44712317-bfef-40e3-e957-47ce965c81be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The finding of this research paper is not explicitly mentioned in the provided context. It is necessary to refer to the actual research paper to obtain the specific finding.\n",
            "[('what is the finding of this research paper', 'The finding of this research paper is not explicitly mentioned in the provided context. It is necessary to refer to the actual research paper to obtain the specific finding.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qns = 'number of research paper refereed in this paper'\n",
        "result, chat_history = ask_with_memory(vector_store, qns, chat_history)\n",
        "print(result['answer'])\n",
        "print(chat_history)"
      ],
      "metadata": {
        "id": "hlHpY92XtSP1",
        "outputId": "05a51350-f698-4b8e-f1b0-eef6110f2e76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the given context, there are at least two research papers referenced in this paper. The references are indicated by the numbers [5] and [13].\n",
            "[('what is the finding of this research paper', 'The finding of this research paper is not explicitly mentioned in the provided context. It is necessary to refer to the actual research paper to obtain the specific finding.'), ('number of research paper refereed in this paper', 'Based on the given context, there are at least two research papers referenced in this paper. The references are indicated by the numbers [5] and [13].')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9sp1YtP5uBYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
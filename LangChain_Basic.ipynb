{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMN+aLMIeNzEjXVYgDM+4mK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rregmi1993/LLM-large-language-model/blob/main/LangChain_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuzQXOCtrAlf",
        "outputId": "6257e69b-dadb-4a7c-85aa-73a62e650bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from -r ./requirement.txt (line 1)) (0.28.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from -r ./requirement.txt (line 2)) (0.0.350)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (from -r ./requirement.txt (line 3)) (2.2.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from -r ./requirement.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from -r ./requirement.txt (line 5)) (0.5.2)\n",
            "Requirement already satisfied: llmx in /usr/local/lib/python3.10/dist-packages (from -r ./requirement.txt (line 6)) (0.0.15a0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai->-r ./requirement.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai->-r ./requirement.txt (line 1)) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai->-r ./requirement.txt (line 1)) (3.9.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (2.0.23)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (0.0.2)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (0.1.0)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (0.0.69)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r ./requirement.txt (line 2)) (8.2.3)\n",
            "Requirement already satisfied: loguru>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r ./requirement.txt (line 3)) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r ./requirement.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r ./requirement.txt (line 3)) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r ./requirement.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r ./requirement.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->-r ./requirement.txt (line 5)) (2023.6.3)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from llmx->-r ./requirement.txt (line 6)) (5.6.3)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (from llmx->-r ./requirement.txt (line 6)) (4.37)\n",
            "Requirement already satisfied: google.auth in /usr/local/lib/python3.10/dist-packages (from llmx->-r ./requirement.txt (line 6)) (2.17.3)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from llmx->-r ./requirement.txt (line 6)) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r ./requirement.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r ./requirement.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r ./requirement.txt (line 1)) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r ./requirement.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r ./requirement.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r ./requirement.txt (line 2)) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r ./requirement.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain->-r ./requirement.txt (line 2)) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain->-r ./requirement.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain->-r ./requirement.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client->-r ./requirement.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->-r ./requirement.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->-r ./requirement.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->-r ./requirement.txt (line 1)) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r ./requirement.txt (line 2)) (3.0.1)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere->llmx->-r ./requirement.txt (line 6)) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from cohere->llmx->-r ./requirement.txt (line 6)) (1.9.1)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere->llmx->-r ./requirement.txt (line 6)) (6.11.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google.auth->llmx->-r ./requirement.txt (line 6)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google.auth->llmx->-r ./requirement.txt (line 6)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google.auth->llmx->-r ./requirement.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer->llmx->-r ./requirement.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain->-r ./requirement.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain->-r ./requirement.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere->llmx->-r ./requirement.txt (line 6)) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google.auth->llmx->-r ./requirement.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r ./requirement.txt (line 2)) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r ./requirement.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y02DirNRsx3C",
        "outputId": "1e5dce9f-aba5-4af4-e46c-5bdfab62e99c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.350\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain --upgrade -q"
      ],
      "metadata": {
        "id": "kQNnMv20I4Ny"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv"
      ],
      "metadata": {
        "id": "zd2k-w4JBV1y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "load_dotenv(find_dotenv(), override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-Zcr1JHBX06",
        "outputId": "e4d67fd1-1c41-49fa-8ae1-ff15d2041d70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "FccnVc53t6ds"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo-1106', temperature=0.8)"
      ],
      "metadata": {
        "id": "87STy06wAwpR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are a AI Researcher\n",
        "              write the user answer about {input_question} in {language} language \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = ['input_question','language'],\n",
        "    template = template )"
      ],
      "metadata": {
        "id": "XLlDi_AABKqm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm= llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "-OmFeJx3CV7j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = chain.run({'input_question': 'Gemini vs chatgpt', 'language': 'English'})"
      ],
      "metadata": {
        "id": "_7eC9HlICpBp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R31RVcYlDKaq",
        "outputId": "64a476b5-db29-4fd7-b181-ca42435d32f3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini and ChatGPT are both powerful AI models, but they have different capabilities. Gemini is a cutting-edge AI model designed specifically for natural language processing and understanding, with a focus on generating human-like responses and engaging in meaningful conversations. On the other hand, ChatGPT is a more general-purpose language model that excels at generating coherent and contextually relevant text based on input prompts. While Gemini may be better suited for interactive conversations and understanding nuanced language, ChatGPT may be more versatile and flexible for a wider range of language generation tasks. Ultimately, the choice between Gemini and ChatGPT depends on the specific requirements and goals of the AI application being developed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#above answer looks like chatGPT is yet to update the Gemini model release and purpose... haha"
      ],
      "metadata": {
        "id": "ZILuIeiTDUyf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Sequential Chain***\n",
        "with sequential chains, we can make a series of calls to one or more LLMs. We can take the output from once chain and use it as the input to another chain.\n",
        "\n",
        "There are two types of sequential chains:\n",
        "1. SimpleSequentialChain\n",
        "2. General form of Sequential chains\n",
        "\n",
        "SimpleSequentialChain:\n",
        "it repersents a series of chians, where each individual chain has a single input and a single output, and the output of one step is used as input to the next.\n"
      ],
      "metadata": {
        "id": "ttzrMBplEyGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain"
      ],
      "metadata": {
        "id": "S-bXipZcEdjM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm1 = OpenAI(model_name=\"text-davinci-003\", temperature=0.8, max_tokens=1024)\n",
        "\n",
        "prompt1=PromptTemplate(\n",
        "    input_variables=['concept'],\n",
        "    template = \"\"\" you are the experienced Data Scientist and python programmer,\n",
        "                    write the function that implements the concept of {concept}\"\"\"\n",
        ")\n",
        "\n",
        "chain1 = LLMChain(llm=llm1, prompt=prompt1)"
      ],
      "metadata": {
        "id": "DlYWzsG8GNa_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm2 = OpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0.8)\n",
        "\n",
        "prompt2=PromptTemplate(\n",
        "    input_variables=['function'],\n",
        "    template = \"\"\" Given the python function {function}, describe it as detailed as possible\"\"\"\n",
        ")\n",
        "\n",
        "chain2 = LLMChain(llm=llm2, prompt=prompt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H9_XmJTHBGE",
        "outputId": "50e37c1a-3ab0-4deb-e905-39245f3789d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/llms/openai.py:248: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/llms/openai.py:1057: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)"
      ],
      "metadata": {
        "id": "BIFl063NHnAh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = overall_chain.run(\"SVM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4-RKgxlH4dw",
        "outputId": "4f5b8e1c-1f88-405b-8af5-0f9e8afafa74"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "\n",
            "#Implementing SVM in Python\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "# define a function for SVM\n",
            "def svm(X,Y):\n",
            "    # calculate number of support vectors\n",
            "    N,M = X.shape\n",
            "    # define the kernel function\n",
            "    def kernel(X,Y):\n",
            "        return np.dot(X,Y.T)\n",
            "    \n",
            "    # Step 1: Initialize the parameters\n",
            "    a = np.zeros(N)\n",
            "    b = 0\n",
            "    L = 0.0001\n",
            "    # Step 2: Calculate the error term\n",
            "    E = np.zeros(N)\n",
            "    for i in range(N):\n",
            "        E[i] = b + np.sum(a * Y * kernel(X, X[i])) - Y[i]\n",
            "    # Step 3: Check the optimality condition\n",
            "    suff_opt = True\n",
            "    for i in range(N):\n",
            "        if (Y[i]*E[i] < -L and a[i] < 0) or (Y[i]*E[i] > L and a[i] > 0):\n",
            "            suff_opt = False\n",
            "            break\n",
            "    \n",
            "    # Step 4: If the optimality condition is satisfied, then the SVM is optimized\n",
            "    if suff_opt:\n",
            "        return a, b\n",
            "    # Step 5: If the optimality condition is not satisfied, then select the alpha which violates it \n",
            "    elif not suff_opt:\n",
            "        max_step = 0\n",
            "        max_i = 0\n",
            "        for i in range(N):\n",
            "            if (Y[i]*E[i] < -L and a[i] < 0) or (Y[i]*E[i] > L and a[i] > 0):\n",
            "                step = abs(E[i] - E[max_i])\n",
            "                if step > max_step:\n",
            "                    max_i = i\n",
            "                    max_step = step\n",
            "        # Step 6: Update the alpha and b\n",
            "        a_old = np.copy(a)\n",
            "        a[max_i] = a[max_i] + Y[max_i]\n",
            "        b1 = b - E[max_i]\n",
            "        b2 = b - E[max_i] - Y[max_i]*kernel(X[max_i],X[max_i])\n",
            "        if a[max_i] > 0 and a[max_i] < L:\n",
            "            b = b1\n",
            "        elif a[max_i] == 0 or a[max_i] == L:\n",
            "            b = (b1 + b2)/2\n",
            "        # Step 7: Update the error term\n",
            "        for i in range(N):\n",
            "            E[i] = b + np.sum(a * Y * kernel(X, X[i])) - Y[i]\n",
            "        return a, b\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mThe given python function is an implementation of Support Vector Machine (SVM) for binary classification. SVM is a supervised learning algorithm that classifies data by finding the hyperplane that best separates the data into different classes.\n",
            "\n",
            "The function `svm` takes two input parameters - X and Y, which represent the feature vectors and corresponding class labels respectively. The function returns the optimized coefficients 'a' and intercept 'b' of the SVM model.\n",
            "\n",
            "Here's a detailed description of the function:\n",
            "\n",
            "1. Calculate the number of support vectors. Support vectors are the data points that lie closest to the decision boundary.\n",
            "\n",
            "2. Define the kernel function. In this case, it uses the linear kernel, which is the dot product between input vectors.\n",
            "\n",
            "3. Initialize the parameters:\n",
            "   - 'a': Coefficients/weights for each support vector\n",
            "   - 'b': Intercept\n",
            "   - 'L': Tolerance parameter\n",
            "\n",
            "4. Calculate the error term 'E' for each support vector. The error represents the difference between the predicted class and the actual class for each data point.\n",
            "\n",
            "5. Check the optimality condition. Whether the current solution satisfies the optimality condition. If yes, the SVM is optimized and the function returns the optimized 'a' and 'b'.\n",
            "\n",
            "6. If the optimality condition is not satisfied, select the alpha (coefficient) which violates it and update the model parameters accordingly:\n",
            "   - Update the alpha and intercept 'b'.\n",
            "   - Update the error term 'E'.\n",
            "\n",
            "7. Return the updated coefficients 'a' and intercept 'b'.\n",
            "\n",
            "The function uses an iterative optimization approach to find the optimal coefficients and intercept for the SVM model by updating the parameters based on the error and the optimality condition.\n",
            "\n",
            "This is a basic implementation of SVM using a simple linear kernel. More sophisticated kernels and optimization algorithms can be used for complex datasets.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`LangChain Agents`**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "684TnCTiKbBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_experimental -q"
      ],
      "metadata": {
        "id": "0xJfHsiuIN6-",
        "outputId": "2002867e-7574-4599-c145-614f1e098f39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m122.9/163.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.0/163.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "KH7szGtLImrh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0)\n",
        "agent_executor = create_python_agent(\n",
        "    llm = llm,\n",
        "    tool = PythonREPLTool(),\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "7R3Xg_tzKh2e"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.run('calculate sqaute root of factorial of 20 and display it with 5 decimal points')"
      ],
      "metadata": {
        "id": "M_JKAhb-L9fl",
        "outputId": "3579befd-4f9a-448f-a2db-5ea15e85ad9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I need to calculate the factorial of 20 and then take the square root of it\n",
            "Action: Python_REPL\n",
            "Action Input: from math import factorial; print(round(factorial(20)**0.5, 5))\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m1559776268.6285\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 1559776268.6285\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1559776268.6285'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.run('calculate 2 ^ 3.3')"
      ],
      "metadata": {
        "id": "ua-So4UtMJ18",
        "outputId": "98822dab-b464-45ef-ecb7-bc8ab4af5ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to use the power operator\n",
            "Action: Python_REPL\n",
            "Action Input: print(2 ** 3.3)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m9.849155306759329\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 9.849155306759329\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'9.849155306759329'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2**3.3"
      ],
      "metadata": {
        "id": "N3oc3rY-MaI0",
        "outputId": "18db607f-20f2-466d-b47a-cf1b62dded13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.849155306759329"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''if you directly ask chatGPT3/4 to calculate the any exponential calculation it will give incorrect result so using langchain allow us to use a tool like python and provide the correct answer'''"
      ],
      "metadata": {
        "id": "UMiR3dZrMfcb",
        "outputId": "0aad816f-3534-4854-c280-1f2c3ba67443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'if you directly ask chatGPT3/4 to calculate the any exponential calculation it will give incorrect result so using langchain allow us to use a tool like python and provide the correct answer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gaWlKoXgNFRq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}